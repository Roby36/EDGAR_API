{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO-DO\n",
    "\n",
    "#### Navigate through comp_facts dictionary locating desired value\n",
    "#### Improve table extraction procedure from filings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 16:40:18,094 - INFO - Request to https://www.sec.gov/files/company_tickers.json returned successfully. Response code: 200\n"
     ]
    }
   ],
   "source": [
    "import ealib\n",
    "import logging\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "import random\n",
    "from typing import Tuple\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Callable\n",
    "import os\n",
    "\n",
    "# Required to identify with EDGAR API\n",
    "req_header = {\"User-Agent\": \"roberto.brera.24@outlook.com\"}\n",
    "\n",
    "# Select rate of requests (< 10)\n",
    "mrps = 8\n",
    "\n",
    "# Select desired logging level\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)\n",
    "\n",
    "# Stores information about all tickers currently available on SEC database\n",
    "tickers_df = ealib.get_tickers_df(req_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_substr =  \"microvis\"\n",
    "\n",
    "query_ticker = ealib.find_title_substring(tickers_df, query_substr).iloc[0]\n",
    "comp_str = f'{query_ticker[\"title\"]} ({query_ticker[\"ticker\"]}, Index {query_ticker.name})'\n",
    "\n",
    "# Retrieve main dictionaries required for this ticker\n",
    "comp_mtd = ealib.get_response_dict(ealib.metadata_url(query_ticker[\"cik_str\"]), req_header, mrps)\n",
    "if comp_mtd is None:\n",
    "    logging.warning(f'Failed to retrieve company metadata for ticker {query_ticker[\"ticker\"]}')\n",
    "comp_facts = ealib.get_response_dict(ealib.companyfacts_url(query_ticker[\"cik_str\"]), req_header, mrps)\n",
    "if comp_facts is None:\n",
    "    logging.warning(f'Failed to retrieve company facts for ticker {query_ticker[\"ticker\"]}')\n",
    "\n",
    "query_ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter by select string contained in select filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_query_str_in_sel_doc(comp_mtd: dict, ticker: pd.DataFrame, query_forms: List[str], query_str: str, req_header, mrps) -> int:\n",
    "\n",
    "    # Fetch the text of latest 10-K or 20-F\n",
    "    select_filings=ealib.filter_filings(\n",
    "        pd.DataFrame.from_dict(comp_mtd[\"filings\"][\"recent\"]), \n",
    "        \"filingDate\", \"form\", \n",
    "        query_forms=query_forms, \n",
    "        max_days=2*360, # Guarantees a full year\n",
    "        min_days=0*360\n",
    "    )\n",
    "\n",
    "    # Check if we have a valid dataframe, or if no filings available\n",
    "    if select_filings is None or select_filings.empty or select_filings.shape[0] == 0:\n",
    "        return -1\n",
    "\n",
    "    curr_doc = select_filings.iloc[0, :] # select first descriptive filing\n",
    "\n",
    "    response = ealib.requests_get_wrp(\n",
    "        ealib.doc_url(\n",
    "            ticker.get(\"cik_str\", \"\"),\n",
    "            curr_doc[\"accessionNumber\"].replace(\"-\",\"\"),\n",
    "            curr_doc[\"primaryDocument\"]\n",
    "        ),\n",
    "        req_header,\n",
    "        mrps=mrps\n",
    "    )\n",
    "\n",
    "    return response.text.lower().count(query_str.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search company fact with refined query string \n",
    "query_forms = [\"10-k\", \"20-f\"]\n",
    "query_str = \" lidar \"\n",
    "out_file = \"matches.txt\"\n",
    "res_df = pd.DataFrame(columns=tickers_df.iloc[0].keys())\n",
    "qso = Counter()\n",
    "\n",
    "for index, curr_ticker in tickers_df.iterrows():\n",
    "    # Retrieve company data for curr_ticker\n",
    "    comp_mtd = ealib.get_response_dict(ealib.metadata_url(curr_ticker[\"cik_str\"]), req_header, mrps)\n",
    "    if comp_mtd is None:\n",
    "        logging.warning(f'Failed to retrieve company metadata for ticker {query_ticker[\"ticker\"]}')\n",
    "        qso[f'-1'] += 1\n",
    "        continue\n",
    "\n",
    "    count = count_query_str_in_sel_doc(comp_mtd, curr_ticker, query_forms, query_str, req_header, mrps)\n",
    "    \n",
    "    if count > 0:\n",
    "        print(f\"Found matching form with count {count}\")\n",
    "        res_df.loc[len(res_df)] = curr_ticker\n",
    "        with open(out_file, 'a') as file:\n",
    "            # Append the text to the file\n",
    "            file.write(f'{curr_ticker[\"ticker\"]}\\n')  # Adding a newline character for formatting\n",
    "\n",
    "    qso[f'{count}'] += 1\n",
    "\n",
    "\n",
    "print(qso)\n",
    "res_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.read_excel(\"lidar_screening.xlsx\")\n",
    "\n",
    "res_df = res_df.drop(columns=\"Unnamed: 0\")\n",
    "\n",
    "# Use .apply to generate new columns\n",
    "\n",
    "res_df[\"Market Cap (USD)\"] = res_df[\"ticker\"].apply(\n",
    "    lambda ticker: ealib.yf_info(ticker, \"marketCap\") * ealib.exch_rate(ealib.yf_info(ticker, \"currency\"), \"USD\")\n",
    ")\n",
    "\n",
    "res_df[\"Last price (USD)\"] = res_df[\"ticker\"].apply(\n",
    "    lambda ticker: yf.Ticker(ticker).history(period=\"1d\")[\"Close\"].iloc[-1] * ealib.exch_rate(ealib.yf_info(ticker, \"currency\"), \"USD\")\n",
    ")\n",
    "\n",
    "res_df.to_excel(\"lidar and ADAS screenings.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Select Filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ealib.download_company_filings(\n",
    "    req_header, mrps, \n",
    "    comp_dir=\"FuboTV 10-Q\", \n",
    "    select_filings=ealib.filter_filings(\n",
    "        pd.DataFrame.from_dict(comp_mtd[\"filings\"][\"recent\"]), \n",
    "        \"filingDate\", \"form\", \n",
    "\n",
    "        query_forms=[\"10\"], \n",
    "        max_days=2*360,\n",
    "        min_days=0*360\n",
    "        \n",
    "        ),\n",
    "    ticker=query_ticker, \n",
    "    write_txt=False, \n",
    "    write_pdf=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting tabular data from downloaded html files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-processing html with bs (not efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First identify all the tags and tag attributes present in teh file\n",
    "filename =  \"Cresud 20-F htmls/20-F/2024.txt\"\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    input_html = file.read()\n",
    "\n",
    "soup = BeautifulSoup(input_html, 'html.parser')\n",
    "\n",
    "unique_tag_names = set()\n",
    "unique_tag_attrs = set()  # Use a set to store attribute names\n",
    "\n",
    "for tag in soup.findAll(True):\n",
    "    unique_tag_names.add(tag.name)  # Collecting unique tag names\n",
    "    unique_tag_attrs.update(tag.attrs.keys())  # Adding attribute names\n",
    "\n",
    "# Print unique tag names and attributes\n",
    "print(\"Unique Tag Names:\", unique_tag_names)\n",
    "print(\"Unique Attributes Used in Tags:\", unique_tag_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then identify tags/ attributes thatw e want to remove\n",
    "tags_to_unwrap = ['span', 'div', 'a', 'br', 'img']\n",
    "tags_to_remove = ['meta', 'title', 'link', 'script', 'style', 'head', 'body', 'html', 'ix:nonfraction', 'ix:nonnumeric']\n",
    "attributes_to_remove = ['style', 'class', 'id', 'name', 'xmlns:xbrli', 'xmlns:xsi', 'xlink:href', 'colspan', 'rowspan', 'href', 'src', 'alt', 'title', 'scheme', 'arcrole', 'role', 'type', 'xmlns', 'xml:lang']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parse and clean up the html from unnecessary tags, storing to separate file\n",
    "example_file_path = \"html_table_example.html\"\n",
    "cleaned_html_filepath = 'cleaned_table.html'\n",
    "\n",
    "# SImplified lists\n",
    "tags_to_unwrap = ['span']\n",
    "attributes_to_remove = ['style']\n",
    "tags_to_remove = []\n",
    "\n",
    "with open(example_file_path, 'r') as file:\n",
    "    full_html = file.read()\n",
    "\n",
    "soup = BeautifulSoup(full_html, 'html.parser')\n",
    "\n",
    "# Removing unnecessary attributes from all tags\n",
    "for tag in soup.findAll(True):\n",
    "    # Check if the tag itself should be completely removed\n",
    "    if tag.name in tags_to_remove:\n",
    "        tag.decompose()  # Removes the tag and its contents completely from the soup\n",
    "    else:\n",
    "        # Clean up attributes\n",
    "        tag.attrs = {key: value for key, value in tag.attrs.items() if key not in attributes_to_remove}\n",
    "\n",
    "# Unwrap specified tags\n",
    "for tag_name in tags_to_unwrap:\n",
    "    for tag in soup.find_all(tag_name):\n",
    "        tag.unwrap()\n",
    "\n",
    "# Assume soup has been initialized and loaded with HTML content\n",
    "for tr in soup.find_all('tr'):\n",
    "    # Iterate over each <td> in the row\n",
    "    for td in tr.find_all('td'):\n",
    "        # Check if <td> is empty: no text and no attributes\n",
    "        if not td.get_text(strip=True) and not td.attrs:\n",
    "            # Remove the empty <td>\n",
    "            td.decompose()\n",
    "\n",
    "# Convert cleaned HTML to string\n",
    "clean_html = str(soup)\n",
    "\n",
    "# Save the cleaned HTML\n",
    "with open(cleaned_html_filepath, 'w') as file:\n",
    "    file.write(clean_html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subcolumns(df: pd.DataFrame, col: pd.Series) -> List:\n",
    "    \"\"\"\n",
    "    Returns a mask, where False means that a column in the dataframe is \n",
    "    a \"subcolumn\" of col, meaning that it contains the same data but more NaN values\n",
    "    \"\"\"\n",
    "    mask = []\n",
    "    for index, dcol in df.items():\n",
    "        is_equal = col.isna() | dcol.isna() | (col == dcol)\n",
    "        mask.append(~((is_equal == 1).all()) or col.isna().sum() >= dcol.isna().sum())\n",
    "    return mask\n",
    "\n",
    "def discard_subcolumns(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Applies the subcolumns function on each column of the dataframe and returns the resulting dataframe\n",
    "    \"\"\"\n",
    "    cum_mask = [True for _ in range(len(df.columns))] # initialize uniformly to True\n",
    "    for index, col in df.items():\n",
    "        mask = subcolumns(df, col)\n",
    "        cum_mask = [cum_mask[i] and mask[i] for i in range(len(cum_mask))] # vectorized AND operation\n",
    "    return df.loc[:, cum_mask]\n",
    "\n",
    "def clean_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" (1) Remove fully NaN rows and columns \"\"\"\n",
    "    res = df.dropna(how='all').dropna(axis=1, how='all')\n",
    "\n",
    "    \"\"\" (2) Remove duplicate columns \"\"\"\n",
    "    df_transposed = res.T\n",
    "    df_transposed = df_transposed[~df_transposed.duplicated()]\n",
    "    res = df_transposed.T\n",
    "\n",
    "    \"\"\" (3) Discard NaN subcolumns \"\"\"\n",
    "    res = discard_subcolumns(res)\n",
    "\n",
    "    \"\"\" (4) TODO: Convert Excel-style numbering to interpretable numbers \"\"\"\n",
    "    res = res.map(lambda x: x.replace('â€”', '0') if isinstance(x, str) else x)  # Replace dashes with zero (assumes dashes mean zero)\n",
    "    res = res.map(lambda x: pd.to_numeric(x.replace(',', '').replace('(', '-').replace(')', '') if isinstance(x, str) else x, errors=\"ignore\")) # TODO: Handle errors explicitly (warning)\n",
    "\n",
    "    \"\"\" TODO: other preprocessing steps \"\"\"\n",
    "    return res\n",
    "\n",
    "def df_contains_substr(df: pd.DataFrame, query_str: str) -> bool:\n",
    "    return  df.map(lambda x: query_str.lower() in str(x).lower()).any().any()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract tables with pd.read_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Extract tables from a list of files \"\"\"\n",
    "filenames: str =  [\n",
    "    #f\"AdecoAgro 20-F html files/20-F/202{i}.html\" for i in range(5)\n",
    "    \"BrasilAgro 20-F html/20-F/2024.txt\"\n",
    "]\n",
    "\n",
    "tables : List[pd.DataFrame] = []\n",
    "\n",
    "for file in filenames:\n",
    "    tables = tables + pd.read_html(file)\n",
    "\n",
    "len(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter tables based on query string containment, then apply cleaning funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First query into raw table text\n",
    "queries: List[str] = [\n",
    "    \"Jato\", \"Xingu\", \"Nova\", \"ETH\"\n",
    "]\n",
    "q2: List[str] = [\n",
    "    \"Regalito\", \"Unagro\", \"Panamby\"\n",
    "]\n",
    "\n",
    "\n",
    "query_tables = [df for df in tables if df_contains_substr_all(df, queries) or df_contains_substr_all(df, q2)]\n",
    "\n",
    "# Then clean smaller list of resulting tables\n",
    "cleaned_tables = clean_tabs(query_tables, clean_cols)\n",
    "len(cleaned_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Indexing the tables (multi-indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survery the structure of the tables\n",
    "row_counter = Counter()\n",
    "col_counter = Counter()\n",
    "for tab in cleaned_tables:\n",
    "    row_counter[f'{tab.shape[0]}'] += 1\n",
    "    col_counter[f'{tab.shape[1]}'] += 1\n",
    "\n",
    "print(f'Row Counter: {row_counter}')\n",
    "print(f'Col Counter: {col_counter}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_num = random.randint(0, len(cleaned_tables) - 1)\n",
    "print(table_num)\n",
    "\n",
    "ex_tab = cleaned_tables[table_num]\n",
    "ex_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dates\n",
    "ex_tab.map(lambda val: pd.to_datetime(val, errors='coerce'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Assume first rows are headers for a hierarchical index \"\"\"\n",
    "\"\"\" Proceed until NOT number OR datetime to dynamically find col_layers\"\"\"\n",
    "col_layers = 2\n",
    "\n",
    "\"\"\" Fix column headers \"\"\"\n",
    "header = pd.MultiIndex.from_arrays([ex_tab.iloc[i] for i in range(col_layers)])\n",
    "ex_tab.columns = header\n",
    "ex_tab = ex_tab.drop(ex_tab.index[0:2])\n",
    "\n",
    "# TO-DO: find way to drop nans from headers/columns safely\n",
    "ex_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Set (row) index to first column \"\"\"\n",
    "ex_tab.set_index(ex_tab.columns[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_tab[\"Farming\", \"Rice\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Other processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate with index\n",
    "# TODO: Improve this very basic refinement method\n",
    "output_tabs : List[pd.DataFrame] = []\n",
    "for query_tab in query_tables:\n",
    "    output_tabs.append(query_tab.iloc[:2, :])\n",
    "    output_tabs.append(query_tab[query_tab[0].str.contains(queries[0], na=False)])\n",
    "\n",
    "# Save to Excel\n",
    "pd.concat(output_tabs).to_excel(\"EBITDA_adj.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selecting desired columns from list of dataframes, and saving each one to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 1\n",
    "cleaned_tables[col].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start displaying, and selecting desired columns\n",
    "\n",
    "desired_cols=[\n",
    "    [0, 2, 4, 7, 11, 14, 16], \n",
    "    [0, 2, 4, 7, 11, 14, 16],\n",
    "]\n",
    "\n",
    "# Preview for a select table\n",
    "cleaned_tables[col].loc[:, desired_cols[col] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Excel\n",
    "with pd.ExcelWriter('brasilagro- land segments.xlsx', engine='openpyxl') as writer:\n",
    "    for i, df in enumerate(cleaned_tables):\n",
    "        # Write each DataFrame to a separate sheet, naming sheets as 'Sheet1', 'Sheet2', etc.\n",
    "        df.loc[:, desired_cols[i]].to_excel(writer, sheet_name=f'Sheet{i+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"html_tests/html_table_example.html\"\n",
    "\n",
    "table = pd.read_html(filepath)[0]\n",
    "\n",
    "clean_table = clean_cols(table)\n",
    "clean_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Further refining methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindex columns according to the date row\n",
    "date_row = 1\n",
    "\n",
    "# Store only first column associated with each date \n",
    "# TODO: Handle this more robustly: we are not guaranteed that the first column encountered will carry the data!\n",
    "# if more columns have the same desired column header, keep only the one column with least NaN values\n",
    "example_table = example_table.loc[:, ~example_table.iloc[date_row].duplicated()]\n",
    "\n",
    "# Reset column headers to desired one\n",
    "example_table.columns = example_table.iloc[date_row]\n",
    "example_table = example_table.drop(example_table.index[:date_row+1])\n",
    "\n",
    "example_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move production up and delete Nan value, reindex\n",
    "# TODO: Generalize this operation as a row re-index, analogously to the column above\n",
    "nan_col = example_table.columns[0]\n",
    "repl_val = example_table[nan_col].iloc[0]\n",
    "\n",
    "copy_column_headers = example_table.columns.to_list()\n",
    "copy_column_headers[0] = repl_val\n",
    "example_table.columns = copy_column_headers\n",
    "\n",
    "# TODO: Hardcoded: either write method to recognize such rows, or just keep them as units (would not be a problem!)\n",
    "example_table = example_table.drop(example_table.index[0]) # drop (in tons) (in tons) etc. row\n",
    "example_table = example_table.set_index(repl_val) \n",
    "example_table.index.name = None  # Remove index name for better displaying\n",
    "example_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for datetimes\n",
    "example_table.applymap(lambda val: pd.to_datetime(val, errors='coerce'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining a company fact using yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by making yf ticker obejct\n",
    "curr_yticker = yf.Ticker(query_ticker[\"ticker\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for particular info keys using search functions\n",
    "ealib.find_dict_key_substr(curr_yticker.info, [\"currency\"])\n",
    "ealib.find_dict_key_substr(curr_yticker.info, [\"cap\"])\n",
    "ealib.find_keys_containing_all_substrs(curr_yticker.info, [\"cash\", \"operating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use encapsulated function to search safely for the company fact, returning None if not found\n",
    "ealib.yf_info(query_ticker[\"ticker\"], \"marketCap\")\n",
    "ealib.yf_info(query_ticker[\"ticker\"], \"currency\")\n",
    "ealib.yf_info(query_ticker[\"ticker\"], \"operatingCashflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate info series for a df of tickers (can be directly stored on source df)\n",
    "marketCap_series = tickers_df[:100][\"ticker\"].apply(lambda x: ealib.yf_info(x, \"marketCap\"))\n",
    "marketCap_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Historical Series for company fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with historical series for shares outstanding\n",
    "\n",
    "start_year: int = 2018\n",
    "end_year: int   = 2023\n",
    "\n",
    "stock_data = yf.download(query_ticker[\"ticker\"], start=f\"{start_year}-01-01\", end=f\"{end_year}-12-31\")\n",
    "shares_out = ealib.yf_info(query_ticker[\"ticker\"], \"sharesOutstanding\")\n",
    "\n",
    "shares_data = pd.DataFrame(index=stock_data.index)\n",
    "shares_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker: str = query_ticker[\"ticker\"]\n",
    "data: str = \"marketCap\"\n",
    "\n",
    "# Fetch historical data for the given period\n",
    "data = yf.download(ticker, start=f\"{start_year}-01-01\", end=f\"{end_year}-12-31\")\n",
    "\n",
    "# Resample the data to yearly frequency and calculate the mean exchange rate for each year\n",
    "yearly_data = data['Close'].resample('Y').mean()\n",
    "\n",
    "# Convert to DataFrame\n",
    "yearly_avg_df = yearly_data.reset_index()\n",
    "yearly_avg_df.columns = ['Year', f'Average Exchange Rate ({from_currency}/{to_currency})']\n",
    "\n",
    "# Display the DataFrame\n",
    "yearly_avg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saved Refined Query Strings for Company Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shares_outstanding_query_str = [\"NumberOfSharesOutstanding\", \"EntityCommonStockSharesOutstanding\", \"CommonStockSharesOutstanding\"]\n",
    "ocf_query_str = [\"NetCashProvidedByUsedInOperatingActivities\", \"CashFlowsFromUsedInOperatingActivities\"]\n",
    "# To use with sufficient = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactive Company Fact Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Company facts notes\n",
    "\"\"\" \n",
    "\"NoncurrentPortionOfNoncurrentNotesAndDebenturesIssued\"\n",
    "\"CurrentNotesAndDebenturesIssuedAndCurrentPortionOfNoncurrentNotesAndDebenturesIssued\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_all_comp_fact_df = False\n",
    "display_short_comp_fact_df = True\n",
    "\n",
    "# If you do not yet have query str -> Broad search\n",
    "query_fact_substr = [\"cash\", \"equiv\"]\n",
    "sufficient = False\n",
    "\n",
    "# If you have query str -> Refined search \n",
    "\"\"\"\n",
    "query_fact_substr = [\"NumberOfSharesOutstanding\", \"EntityCommonStockSharesOutstanding\", \"CommonStockSharesOutstanding\"] # Refined list\n",
    "sufficient = True\n",
    "\"\"\"\n",
    "\n",
    "# Navigate company facts dictionary and find all matches to query company fact\n",
    "res = ealib.comp_facts_df(\n",
    "    comp_facts,\n",
    "    query_fact_substr, \n",
    "    sufficient,\n",
    ")\n",
    "for units, as_key, match_fact, comp_fact_df in res:\n",
    "    logging.info(f'Found company fact matching {query_fact_substr}: as_key = {as_key}, match_fact = {match_fact}, units = {units}')\n",
    "    logging.info(f'\\n\\t{comp_fact_df}') if print_all_comp_fact_df else None\n",
    "if res: # If only one company fact required, select closest match by choosing shortest matching string\n",
    "    res_units, res_as_key, res_match_fact, res_comp_fact_df = min(res, key=lambda x: len(x[2]))\n",
    "    logging.info(f'Shortest company fact match:\\n\\t res_as_key = {res_as_key}, res_match_fact = {res_match_fact}, res_units = {res_units}')\n",
    "    display(res_comp_fact_df) if display_short_comp_fact_df else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze presence of selected company fact across tickers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search company fact with refined query string \n",
    "query_fact_substr = [\"DepreciationAndAmortisation\", \"DepreciationDepletionAndAmortization\"] \n",
    "sufficient = True\n",
    "print_short_comp_fact_df = False\n",
    "num_tests = 100\n",
    "\n",
    "len_counter = Counter()\n",
    "for _ in range(num_tests):\n",
    "    random_number = random.randint(1, 10000)\n",
    "    curr_ticker = tickers_df.iloc[random_number]\n",
    "    comp_str = f'ticker number: {random_number}; ticker: {curr_ticker[\"ticker\"]}; company title: {curr_ticker[\"title\"]}'\n",
    "\n",
    "    # Request comp facts dictionary\n",
    "    comp_facts = ealib.get_response_dict(ealib.companyfacts_url(curr_ticker[\"cik_str\"]), req_header, mrps)\n",
    "    if comp_facts is None:\n",
    "            logging.warning(f'Failed request when attempting to retrieve company facts for {comp_str}')\n",
    "            len_counter['failed reqs'] += 1\n",
    "\n",
    "    # Extract desired company fact\n",
    "    res = ealib.comp_facts_df(\n",
    "        comp_facts,\n",
    "        query_fact_substr, \n",
    "        sufficient,\n",
    "    )\n",
    "    # Record the number of matches\n",
    "    len_counter[f'{len(res)}'] += 1\n",
    "\n",
    "    for units, as_key, match_fact, comp_fact_df in res:\n",
    "        logging.info(f'Found company fact matching {query_fact_substr}: as_key = {as_key}, match_fact = {match_fact}, units = {units} with {comp_str}')\n",
    "    if res: # Now extract tuple with shortest match_fact\n",
    "        res_units, res_as_key, res_match_fact, res_comp_fact_df = min(res, key=lambda x: len(x[2]))\n",
    "        logging.info(f'Shortest match fact tuple selected:\\n\\t res_as_key = {res_as_key}, res_match_fact = {res_match_fact}, res_units = {res_units} for test with {comp_str}')\n",
    "        logging.info(res_comp_fact_df) if print_short_comp_fact_df else None\n",
    "        \n",
    "len_counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphing a Company Fact Over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select Filings for Graph's Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Run Interactive Company Fact Search first, establishing following variables\n",
    "    res_units, res_as_key, res_match_fact, res_comp_fact_df\n",
    "\"\"\"\n",
    "\n",
    "# First check the filings on which graph will be based\n",
    "filing_date_col = \"end\"\n",
    "y_axis_col = \"val\"\n",
    "max_days = 360\n",
    "\n",
    "select_filings_df = ealib.filter_filings(res_comp_fact_df, filing_date_col=filing_date_col, form_col=\"form\", query_forms=[\"\"], max_days=max_days)\n",
    "select_filings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))  # Set the figure size (optional)\n",
    "plt.plot( select_filings_df[filing_date_col], select_filings_df[y_axis_col], marker='o')  # Line plot with markers\n",
    "plt.title(f'{res_match_fact} for {comp_str}')  # Adding a title to the graph\n",
    "plt.xlabel('Filing end date')  # Label for the x-axis\n",
    "plt.ylabel(f'Reported Number of {res_match_fact}')  # Label for the y-axis\n",
    "plt.grid(True)  # Enable grid for easier readability\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating cash burn rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Calculates average cash burn daily rate, according to all SEC filing data available over the specified period\n",
    "\"\"\"\n",
    "min_days = 0\n",
    "max_days = 360\n",
    "query_forms = [\"\"]\n",
    "filing_date_col = \"filed\"\n",
    "\n",
    "res = ealib.comp_facts_df(\n",
    "    comp_facts,\n",
    "    [\"NetCashProvidedByUsedInOperatingActivities\", \"CashFlowsFromUsedInOperatingActivities\"], \n",
    "    True\n",
    ")\n",
    "if res:\n",
    "    res_units, res_as_key, res_match_fact, res_comp_fact_df = min(res, key=lambda x: len(x[2]))\n",
    "    ocf_df_filt = ealib.filter_filings(res_comp_fact_df, filing_date_col=filing_date_col, form_col=\"form\", query_forms=query_forms, max_days=max_days, min_days=min_days)\n",
    "    display(f'Average daily OCF burn rate {ealib.ocf_average_daily_burn_rate(ocf_df_filt)}, with units {res_units} and company fact {res_match_fact}, under accounting standards {res_as_key}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting between currencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forex_ticker(from_currency: str, to_currency: str) -> str:\n",
    "    return f\"{from_currency}{to_currency}=X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_currency = \"USD\"\n",
    "to_currency = \"USD\"\n",
    "\n",
    "ealib.yf_info(forex_ticker(from_currency, to_currency), \"previousClose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mean exchange rate for a given period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean exchange rate throughout the year\n",
    "start_year: int = 2018\n",
    "end_year: int   = 2023\n",
    "\n",
    "from_currency: str = \"ARS\"\n",
    "to_currency: str = \"USD\"\n",
    "\n",
    "# Fetch historical data for the given period\n",
    "data = yf.download(forex_ticker(from_currency, to_currency), start=f\"{start_year}-01-01\", end=f\"{end_year}-12-31\")\n",
    "\n",
    "# Resample the data to yearly frequency and calculate the mean exchange rate for each year\n",
    "yearly_data = data['Close'].resample('YE').mean()\n",
    "\n",
    "# Convert to DataFrame\n",
    "yearly_avg_df = yearly_data.reset_index()\n",
    "yearly_avg_df.columns = ['Year', f'Average Exchange Rate ({from_currency}/{to_currency})']\n",
    "\n",
    "# Display the DataFrame\n",
    "yearly_avg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing-of-year exchange rate\n",
    "yearly_data = data['Close'].resample('YE').last()\n",
    "yearly_avg_df = yearly_data.reset_index()\n",
    "yearly_avg_df.columns = ['Year', f'Average Exchange Rate ({from_currency}/{to_currency})']\n",
    "yearly_avg_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
